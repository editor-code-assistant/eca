{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#eca-editor-code-assistant","title":"ECA (Editor Code Assistant)","text":"eca-emacs eca-vscode eca-intellij <p> installation \u2022   features \u2022   configuration \u2022   models \u2022   protocol troubleshooting </p> <ul> <li> Editor-agnostic: protocol for any editor to integrate.</li> <li> Single configuration: Configure eca making it work the same in any editor via global or local configs.</li> <li> Chat interface: ask questions, review code, work together to code.</li> <li> Agentic: let LLM work as an agent with its native tools and MCPs you can configure.</li> <li> Context: support: giving more details about your code to the LLM, including MCP resources and prompts.</li> <li> Multi models: Login to OpenAI, Anthropic, Copilot, Ollama local models and many more.</li> <li> OpenTelemetry: Export metrics of tools, prompts, server usage.</li> </ul>"},{"location":"#rationale","title":"Rationale","text":"<p>A Free and OpenSource editor-agnostic tool that aims to easily link LLMs &lt;-&gt; Editors, giving the best UX possible for AI pair programming using a well-defined protocol. The server is written in Clojure and heavily inspired by the LSP protocol which is a success case for this kind of integration.</p> <p>The protocol makes easier for other editors integrate and having a server in the middle helps adding more features quickly, some examples: - Tool call management - Multiple LLM interaction  - Telemetry of features usage - Single way to configure for any editor - Same UX, easy to onboard people and teams. </p> <p>With the LLMs models race, the differences between them tend to be irrelevant in the future, but UX on how to edit code or plan changes is something that will exist, ECA helps editors focus on that.</p> <p>How it works: Editors spawn the server via <code>eca server</code> and communicate via stdin/stdout, similar to LSPs. Supported editors already download latest server on start and require no extra configuration.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#1-install-the-editor-plugin","title":"1. Install the editor plugin","text":"<p>Install the plugin for your editor and ECA server will be downloaded and started automatically:</p> <ul> <li>Emacs</li> <li>VsCode</li> <li>Vim</li> <li>Intellij</li> </ul>"},{"location":"#2-set-up-your-first-model","title":"2. Set up your first model","text":"<p>To use ECA, you need to configure at least one model / provider (tip: Github Copilot offer free models!). See the Models documentation for detailed instructions:</p> <ol> <li>Type in the chat <code>/login</code>.</li> <li>Chose your provider</li> <li>Follow the steps to configure the key or auth for your provider.</li> </ol> <p>or configure manually.</p> <p>Note: For other providers or custom models, see the custom providers documentation.</p>"},{"location":"#3-start-chatting","title":"3. Start chatting","text":"<p>Once your model is configured, you can start using ECA's chat interface in your editor to ask questions, review code, and work together on your project.</p> <p>Type <code>/init</code> to ask ECA to create/update a AGENTS.md file which will help ECA on next iterations have good context about your project standards.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Check the planned work here.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome, please open an issue for discussion or a pull request. For developer details, check development docs.</p>"},{"location":"#support-the-project","title":"Support the project \ud83d\udc96","text":"<p>Consider sponsoring the project to help grow faster, the support helps to keep the project going, being updated and maintained!</p> <p>These are all the incredible people who helped make ECA better!</p> <p></p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":"<ul> <li>Improved flaky test #150</li> <li>Obfuscate env vars in /doctor.</li> <li>Bump clj-otel to 0.2.10</li> <li>Rename $ARGS to $ARGUMENTS placeholder alias for custom commands.</li> </ul>"},{"location":"CHANGELOG/#0661","title":"0.66.1","text":"<ul> <li>Improve plan behavior prompt. #139</li> </ul>"},{"location":"CHANGELOG/#0660","title":"0.66.0","text":"<ul> <li>Add support for secrets stored in authinfo and netrc files</li> <li>Added tests for stopping concurrent tool calls. #147</li> <li>Improve logging.</li> <li>Improve performance of <code>chat/queryContext</code>.</li> </ul>"},{"location":"CHANGELOG/#0650","title":"0.65.0","text":"<ul> <li>Added ability to cancel tool calls. Only the shell tool currently. #145</li> <li>Bump mcp java sdk to 0.14.1.</li> <li>Improve json output for tools that output json.</li> </ul>"},{"location":"CHANGELOG/#0641","title":"0.64.1","text":"<ul> <li>Fix duplicated arguments on <code>toolCallPrepare</code> for openai-chat API models. https://github.com/editor-code-assistant/eca-emacs/issues/56</li> </ul>"},{"location":"CHANGELOG/#0640","title":"0.64.0","text":"<ul> <li>Add <code>server</code> to tool call messages.</li> </ul>"},{"location":"CHANGELOG/#0633","title":"0.63.3","text":"<ul> <li>Fix last word going after tool call for openai-chat API.</li> </ul>"},{"location":"CHANGELOG/#0632","title":"0.63.2","text":"<ul> <li>Fix retrocompatibility with some models not working with openai-chat like deepseek.</li> </ul>"},{"location":"CHANGELOG/#0631","title":"0.63.1","text":"<ul> <li>Add <code>gpt-5-codex</code> model as default for <code>openai</code> provider.</li> </ul>"},{"location":"CHANGELOG/#0630","title":"0.63.0","text":"<ul> <li>Support \"accept and remember\" tool call per session and name.</li> <li>Avoid generating huge chat titles.</li> </ul>"},{"location":"CHANGELOG/#0621","title":"0.62.1","text":"<ul> <li>Add <code>claude-sonnet-4.5</code> for github-copilot provider.</li> <li>Add <code>prompt-received</code> metric.</li> </ul>"},{"location":"CHANGELOG/#0620","title":"0.62.0","text":"<ul> <li>Use a default of 32k tokens for max_tokens in openai-chat API.</li> <li>Improve rejection prompt for tool calls.</li> <li>Use <code>max_completion_tokens</code> instead of <code>max_tokens</code> in openai-chat API.</li> <li>Support context/tokens usage/cost for openai-chat API.</li> <li>Support <code>anthropic/claude-sonnet-4.5</code> by default.</li> </ul>"},{"location":"CHANGELOG/#0611","title":"0.61.1","text":"<ul> <li>More tolerant whitespace handling after <code>data:</code>.</li> <li>Fix login for google provider. #134</li> </ul>"},{"location":"CHANGELOG/#0610","title":"0.61.0","text":"<ul> <li>Fix chat titles not working for some providers.</li> <li>Enable reasoning for google models.</li> <li>Support reasoning blocks in models who use openai-chat api.</li> </ul>"},{"location":"CHANGELOG/#0600","title":"0.60.0","text":"<ul> <li>Support google gemini as built-in models. #50</li> </ul>"},{"location":"CHANGELOG/#0590","title":"0.59.0","text":"<ul> <li>Deprecate repoMap context, will be removed in the future.</li> <li>After lots of tunnings and improvements, the repoMap is no longer relevant as <code>eca_directory_tree</code> provides similar and more specific view for LLM to use.</li> <li>Support <code>toolCall shellCommand summaryMaxLength</code> to configure UX of command length. #130</li> </ul>"},{"location":"CHANGELOG/#0582","title":"0.58.2","text":"<ul> <li>Fix MCP prompt for native image.</li> </ul>"},{"location":"CHANGELOG/#0581","title":"0.58.1","text":"<ul> <li>Improve progress notification when tool is running.</li> </ul>"},{"location":"CHANGELOG/#0580","title":"0.58.0","text":"<ul> <li>Bump MCP java sdk to 0.13.1</li> <li>Improve MCP logs on stderr.</li> <li>Support tool call rejection with reasons inputed by user. #127</li> </ul>"},{"location":"CHANGELOG/#0570","title":"0.57.0","text":"<ul> <li>Greatly reduce token consuming of <code>eca_directory_tree</code></li> <li>Ignoring files in gitignore</li> <li>Improving tool output for LLM removing token consuming chars.</li> </ul>"},{"location":"CHANGELOG/#0564","title":"0.56.4","text":"<ul> <li>Fix renew oauth tokens when it expires in the same session.</li> </ul>"},{"location":"CHANGELOG/#0563","title":"0.56.3","text":"<ul> <li>Fix metrics exception when saving to db.</li> </ul>"},{"location":"CHANGELOG/#0562","title":"0.56.2","text":"<ul> <li>Fix db exception.</li> </ul>"},{"location":"CHANGELOG/#0561","title":"0.56.1","text":"<ul> <li>Fix usage reporting.</li> </ul>"},{"location":"CHANGELOG/#0560","title":"0.56.0","text":"<ul> <li>Return new chat metadata content.</li> <li>Add chat title via prompt to LLM.</li> </ul>"},{"location":"CHANGELOG/#0550","title":"0.55.0","text":"<ul> <li>Add support for Opentelemetry via <code>otlp</code> config.</li> <li>Export metrics of server tasks, tool calls, prompts, resources.</li> </ul>"},{"location":"CHANGELOG/#0544","title":"0.54.4","text":"<ul> <li>Use jsonrpc4clj instead of lsp4clj.</li> <li>Bump graalvm to 24 and java to 24 improving native binary perf.</li> </ul>"},{"location":"CHANGELOG/#0543","title":"0.54.3","text":"<ul> <li>Avoid errors on multiple same MCP server calls in parallel.</li> </ul>"},{"location":"CHANGELOG/#0542","title":"0.54.2","text":"<ul> <li>Fix openai cache tokens cost calculation.</li> </ul>"},{"location":"CHANGELOG/#0541","title":"0.54.1","text":"<ul> <li>Improve welcome message.</li> </ul>"},{"location":"CHANGELOG/#0540","title":"0.54.0","text":"<ul> <li>Improve large file handling in <code>read-file</code> tool:</li> <li>Replace basic truncation notice with detailed line range information and next-step instructions.</li> <li>Allow users to customize default line limit through <code>tools.readFile.maxLines</code> configuration (keep the current 2000 as default).</li> <li>Moved the future in :on-tools-called and stored it in the db. #119</li> <li>Support <code>compactPromptFile</code> config.</li> <li>Fix tools not being listed for servers using mcp-remote.</li> </ul>"},{"location":"CHANGELOG/#0530","title":"0.53.0","text":"<ul> <li>Add <code>/compact</code> command to summarize the current conversation helping reduce context size.</li> <li>Add support for images as contexts.</li> </ul>"},{"location":"CHANGELOG/#0520","title":"0.52.0","text":"<ul> <li>Support http-streamable http servers (non auth support for now)</li> <li>Fix promtps that send assistant messages not working for anthropic.</li> </ul>"},{"location":"CHANGELOG/#0513","title":"0.51.3","text":"<ul> <li>Fix manual anthropic login to save credentials in global config instead of cache.</li> </ul>"},{"location":"CHANGELOG/#0512","title":"0.51.2","text":"<ul> <li>Minor log improvement of failed to start MCPs.</li> </ul>"},{"location":"CHANGELOG/#0511","title":"0.51.1","text":"<ul> <li>Bump mcp java sdk to 1.12.1.</li> <li>Fix mcp servers default timeout from 20s -&gt; 60s.</li> </ul>"},{"location":"CHANGELOG/#0510","title":"0.51.0","text":"<ul> <li>Support timeout on <code>eca_shell_command</code> with default to 1min.</li> <li>Support <code>@cursor</code> context representing the current editor cursor position. #103</li> </ul>"},{"location":"CHANGELOG/#0502","title":"0.50.2","text":"<ul> <li>Fix setting the <code>web-search</code> capability in the relevant models</li> <li>Fix summary text for tool calls using <code>openai-chat</code> api.</li> </ul>"},{"location":"CHANGELOG/#0501","title":"0.50.1","text":"<ul> <li>Bump mcp-java-sdk to 0.12.0.</li> </ul>"},{"location":"CHANGELOG/#0500","title":"0.50.0","text":"<ul> <li>Added missing parameters to <code>toolCallRejected</code> where possible.  PR #109</li> <li>Improve plan prompt present plan step.</li> <li>Add custom behavior configuration support. #79</li> <li>Behaviors can now define <code>defaultModel</code>, <code>disabledTools</code>, <code>systemPromptFile</code>, and <code>toolCall</code> approval rules.</li> <li>Built-in <code>agent</code> and <code>plan</code> behaviors are pre-configured.</li> <li>Replace <code>systemPromptTemplateFile</code> with <code>systemPromptFile</code> for complete prompt files instead of templates.</li> <li>Remove <code>nativeTools</code> configuration in favor of <code>toolCall</code> approval and <code>disabledTools</code>.</li> <li>Native tools are now always enabled by default, controlled via <code>disabledTools</code> and <code>toolCall</code> approval.</li> </ul>"},{"location":"CHANGELOG/#0490","title":"0.49.0","text":"<ul> <li>Add <code>totalTimeMs</code> to reason and toolCall content blocks.</li> </ul>"},{"location":"CHANGELOG/#0480","title":"0.48.0","text":"<ul> <li>Add nix flake build.</li> <li>Stop prompt does not change the status of the last running toolCall. #65</li> <li>Add <code>toolCallRunning</code> content to <code>chat/contentReceived</code>.</li> </ul>"},{"location":"CHANGELOG/#0470","title":"0.47.0","text":"<ul> <li>Support more providers login via <code>/login</code>.</li> <li>openai</li> <li>openrouter</li> <li>deepseek</li> <li>azure</li> <li>z-ai</li> </ul>"},{"location":"CHANGELOG/#0460","title":"0.46.0","text":"<ul> <li>Remove the need to pass <code>requestId</code> on prompt messages.</li> <li>Support empty <code>/login</code> command to ask what provider to login.</li> </ul>"},{"location":"CHANGELOG/#0450","title":"0.45.0","text":"<ul> <li>Support user configured custom tools via <code>customTools</code> config. #92</li> <li>Fix default approval for read only tools to be <code>allow</code> instead of <code>ask</code>.</li> </ul>"},{"location":"CHANGELOG/#0441","title":"0.44.1","text":"<ul> <li>Fix renew token regression.</li> <li>Improve error feedback when failed to renew token.</li> </ul>"},{"location":"CHANGELOG/#0440","title":"0.44.0","text":"<ul> <li>Support <code>deny</code> tool calls via <code>toolCall approval deny</code> setting.</li> </ul>"},{"location":"CHANGELOG/#0431","title":"0.43.1","text":"<ul> <li>Safely rename <code>default*</code> -&gt; <code>select*</code> in <code>config/updated</code>.</li> </ul>"},{"location":"CHANGELOG/#0430","title":"0.43.0","text":"<ul> <li>Support <code>chat/selectedBehaviorChanged</code> client notification.</li> <li>Update models according with supported models given its auth or key/url configuration.</li> <li>Return models only authenticated or logged in avoid too much models on UI that won't work.</li> </ul>"},{"location":"CHANGELOG/#0420","title":"0.42.0","text":"<ul> <li>New server notification <code>config/updated</code> used to notify clients when a relevant config changed (behaviors, models etc).</li> <li>Deprecate info inside <code>initialize</code> response, clients should use <code>config/updated</code> now.</li> </ul>"},{"location":"CHANGELOG/#0410","title":"0.41.0","text":"<ul> <li>Improve anthropic extraPayload requirement when adding models.</li> <li>Add message to when config failed to be parsed.</li> <li>Fix context completion for workspaces that are not git. #98</li> <li>Fix session tokens calculation.</li> </ul>"},{"location":"CHANGELOG/#0400","title":"0.40.0","text":"<ul> <li>Drop <code>agentFileRelativePath</code> in favor of behaviors customizations in the future.</li> <li>Unwrap <code>chat</code> config to be at root level.</li> <li>Fix token expiration for copilot and anthropic.</li> <li>Considerably improve toolCall approval / permissions config.</li> <li>Now with thave multiple optiosn to ask or allow tool calls, check config section.</li> </ul>"},{"location":"CHANGELOG/#0390","title":"0.39.0","text":"<ul> <li>Fix session-tokens in usage notifications.</li> <li>Support context limit on usage notifications.</li> <li>Fix session/message tokens calculation.</li> </ul>"},{"location":"CHANGELOG/#0383","title":"0.38.3","text":"<ul> <li>Fix anthropic token renew.</li> </ul>"},{"location":"CHANGELOG/#0382","title":"0.38.2","text":"<ul> <li>Fix command prompts to allow args with spaces between quotes.</li> <li>Fix anthropic token renew when expires.</li> </ul>"},{"location":"CHANGELOG/#0381","title":"0.38.1","text":"<ul> <li>Fix graalvm properties.</li> </ul>"},{"location":"CHANGELOG/#0380","title":"0.38.0","text":"<ul> <li>Improve plan-mode (prompt + eca_preview_file_change tool) #94</li> <li>Add fallback for matching / editing text in files #94</li> </ul>"},{"location":"CHANGELOG/#0370","title":"0.37.0","text":"<ul> <li>Require approval for <code>eca_shell_command</code> if running outside workspace folders.</li> <li>Fix anthropic subscription.</li> </ul>"},{"location":"CHANGELOG/#0365","title":"0.36.5","text":"<ul> <li>Fix pricing for models being case insensitive on its name when checking capabilities.</li> </ul>"},{"location":"CHANGELOG/#0364","title":"0.36.4","text":"<ul> <li>Improve api url error message when not configured.</li> </ul>"},{"location":"CHANGELOG/#0363","title":"0.36.3","text":"<ul> <li>Fix <code>anthropic/claude-3-5-haiku-20241022</code> model.</li> <li>Log json error parsing in configs.</li> </ul>"},{"location":"CHANGELOG/#0362","title":"0.36.2","text":"<ul> <li>Add login providers and server command to <code>/doctor</code>.</li> </ul>"},{"location":"CHANGELOG/#0361","title":"0.36.1","text":"<ul> <li>Improved the <code>eca_directory_tree</code> tool. #82</li> </ul>"},{"location":"CHANGELOG/#0360","title":"0.36.0","text":"<ul> <li>Support relative contexts additions via <code>~</code>, <code>./</code> <code>../</code> and <code>/</code>. #61</li> </ul>"},{"location":"CHANGELOG/#0350","title":"0.35.0","text":"<ul> <li>Anthropic subscription support, via <code>/login anthropic</code> command. #57</li> </ul>"},{"location":"CHANGELOG/#0342","title":"0.34.2","text":"<ul> <li>Fix copilot requiring login in different workspaces.</li> </ul>"},{"location":"CHANGELOG/#0341","title":"0.34.1","text":"<ul> <li>Fix proxy exception. #73</li> </ul>"},{"location":"CHANGELOG/#0340","title":"0.34.0","text":"<ul> <li>Support custom UX details/summary for MCP tools. #67</li> <li>Support clojureMCP tools diff for file changes.</li> </ul>"},{"location":"CHANGELOG/#0330","title":"0.33.0","text":"<ul> <li>Fix reasoning titles in thoughts blocks for openai-responses.</li> <li>Fix hanging LSP diagnostics requests</li> <li>Add <code>lspTimeoutSeconds</code> to config</li> <li>Support <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> env vars for LLM request via proxies. #73</li> </ul>"},{"location":"CHANGELOG/#0324","title":"0.32.4","text":"<ul> <li>Disable <code>eca_plan_edit_file</code> in plan behavior until better idea on what plan behavior should do.</li> </ul>"},{"location":"CHANGELOG/#0323","title":"0.32.3","text":"<ul> <li>Consider <code>AGENTS.md</code> instead of <code>AGENT.md</code>, following the https://agents.md standard.</li> </ul>"},{"location":"CHANGELOG/#0322","title":"0.32.2","text":"<ul> <li>Fix option to set default chat behavior from config via <code>chat defaultBehavior</code>. #71</li> </ul>"},{"location":"CHANGELOG/#0321","title":"0.32.1","text":"<ul> <li>Fix support for models with <code>/</code> in the name like Openrouter ones.</li> </ul>"},{"location":"CHANGELOG/#0320","title":"0.32.0","text":"<ul> <li>Refactor config for better UX and understanding:</li> <li>Move <code>models</code> to inside <code>providers</code>.</li> <li>Make <code>customProviders</code> compatible with <code>providers</code>. models need to be a map now, not a list.</li> </ul>"},{"location":"CHANGELOG/#0310","title":"0.31.0","text":"<ul> <li>Update copilot models</li> <li>Drop uneeded <code>ollama useTools</code> and <code>ollama think</code> configs.</li> <li>Refactor configs for config providers unification.</li> <li><code>&lt;provider&gt;ApiKey</code> and <code>&lt;providerApiUrl&gt;</code> now live in <code>:providers \"&lt;provider&gt;\" :key</code>.</li> <li>Move <code>defaultModel</code> config from customProvider to root.</li> </ul>"},{"location":"CHANGELOG/#0300","title":"0.30.0","text":"<ul> <li>Add <code>/login</code> command to login to providers</li> <li>Add Github Copilot models support with login.</li> </ul>"},{"location":"CHANGELOG/#0292","title":"0.29.2","text":"<ul> <li>Add <code>/doctor</code> command to help with troubleshooting</li> </ul>"},{"location":"CHANGELOG/#0291","title":"0.29.1","text":"<ul> <li>Fix args streaming in toolCallPrepare to not repeat the args. https://github.com/editor-code-assistant/eca-nvim/issues/28</li> </ul>"},{"location":"CHANGELOG/#0290","title":"0.29.0","text":"<ul> <li>Add editor tools to retrieve information like diagnostics. #56</li> </ul>"},{"location":"CHANGELOG/#0280","title":"0.28.0","text":"<ul> <li>Change api for custom providers to support <code>openai-responses</code> instead of just <code>openai</code>, still supporting <code>openai</code> only.</li> <li>Add limit to repoMap with default of 800 total entries and 50 per dir. #35</li> <li>Add support for OpenAI Chat Completions API for broad third-party model support.</li> <li>A new <code>openai-chat</code> custom provider <code>api</code> type was added to support any provider using the standard OpenAI <code>/v1/chat/completions</code> endpoint.</li> <li>This enables easy integration with services like OpenRouter, Groq, DeepSeek, Together AI, and local LiteLLM instances.</li> </ul>"},{"location":"CHANGELOG/#0270","title":"0.27.0","text":"<ul> <li>Add support for auto read <code>AGENT.md</code> from workspace root and global eca dir, considering as context for chat prompts.</li> <li>Add <code>/prompt-show</code> command to show ECA prompt sent to LLM.</li> <li>Add <code>/init</code> command to ask LLM to create/update <code>AGENT.md</code> file.</li> </ul>"},{"location":"CHANGELOG/#0263","title":"0.26.3","text":"<ul> <li>breaking: Replace configs <code>ollama host</code> and <code>ollama port</code> with <code>ollamaApiUrl</code>.</li> </ul>"},{"location":"CHANGELOG/#0262","title":"0.26.2","text":"<ul> <li>Fix <code>chat/queryContext</code> to not return already added contexts</li> <li>Fix some MCP prompts that didn't work.</li> </ul>"},{"location":"CHANGELOG/#0261","title":"0.26.1","text":"<ul> <li>Fix anthropic api for custom providers.</li> <li>Support customize completion api url via custom providers.</li> </ul>"},{"location":"CHANGELOG/#0260","title":"0.26.0","text":"<ul> <li>Support manual approval for specific tools. #44</li> </ul>"},{"location":"CHANGELOG/#0250","title":"0.25.0","text":"<ul> <li>Improve plan-mode to do file changes with diffs.</li> </ul>"},{"location":"CHANGELOG/#0243","title":"0.24.3","text":"<ul> <li>Fix initializationOptions config merge.</li> <li>Fix default claude model.</li> </ul>"},{"location":"CHANGELOG/#0242","title":"0.24.2","text":"<ul> <li>Fix some commands not working.</li> </ul>"},{"location":"CHANGELOG/#0241","title":"0.24.1","text":"<ul> <li>Fix build</li> </ul>"},{"location":"CHANGELOG/#0240","title":"0.24.0","text":"<ul> <li>Get models and configs from models.dev instead of hardcoding in eca.</li> <li>Allow custom models addition via <code>models &lt;modelName&gt;</code> config.</li> <li>Add <code>/resume</code> command to resume previous chats.</li> <li>Support loading system prompts from a file.</li> <li>Fix model name parsing.</li> </ul>"},{"location":"CHANGELOG/#0231","title":"0.23.1","text":"<ul> <li>Fix openai reasoning not being included in messages.</li> </ul>"},{"location":"CHANGELOG/#0230","title":"0.23.0","text":"<ul> <li>Support parallel tool call.</li> </ul>"},{"location":"CHANGELOG/#0220","title":"0.22.0","text":"<ul> <li>Improve <code>eca_shell_command</code> to handle better error outputs.</li> <li>Add summary for eca commands via <code>summary</code> field on tool calls.</li> </ul>"},{"location":"CHANGELOG/#0211","title":"0.21.1","text":"<ul> <li>Default to gpt-5 instead of o4-mini when openai-api-key found.</li> <li>Considerably improve <code>eca_shell_command</code> to fix args parsing + git/PRs interactions.</li> </ul>"},{"location":"CHANGELOG/#0210","title":"0.21.0","text":"<ul> <li>Fix openai skip streaming response corner cases.</li> <li>Allow override payload of any LLM provider.</li> </ul>"},{"location":"CHANGELOG/#0200","title":"0.20.0","text":"<ul> <li>Support custom commands via md files in <code>~/.config/eca/commands/</code> or <code>.eca/commands/</code>.</li> </ul>"},{"location":"CHANGELOG/#0190","title":"0.19.0","text":"<ul> <li>Support <code>claude-opus-4-1</code> model.</li> <li>Support <code>gpt-5</code>, <code>gpt-5-mini</code>, <code>gpt-5-nano</code> models.</li> </ul>"},{"location":"CHANGELOG/#0180","title":"0.18.0","text":"<ul> <li>Replace <code>chat</code> behavior with <code>plan</code>.</li> </ul>"},{"location":"CHANGELOG/#0172","title":"0.17.2","text":"<ul> <li>fix query context refactor</li> </ul>"},{"location":"CHANGELOG/#0171","title":"0.17.1","text":"<ul> <li>Avoid crash MCP start if doesn't support some capabilities.</li> <li>Improve tool calling to avoid stop LLM loop if any exception happens.</li> </ul>"},{"location":"CHANGELOG/#0170","title":"0.17.0","text":"<ul> <li>Add <code>/repo-map-show</code> command. #37</li> </ul>"},{"location":"CHANGELOG/#0160","title":"0.16.0","text":"<ul> <li>Support custom system prompts via config <code>systemPromptTemplate</code>.</li> <li>Add support for file change diffs on <code>eca_edit_file</code> tool call.</li> <li>Fix response output to LLM when tool call is rejected.</li> </ul>"},{"location":"CHANGELOG/#0153","title":"0.15.3","text":"<ul> <li>Rename <code>eca_list_directory</code> to <code>eca_directory_tree</code> tool for better overview of project files/dirs.</li> </ul>"},{"location":"CHANGELOG/#0152","title":"0.15.2","text":"<ul> <li>Improve <code>eca_edit_file</code> tool for better usage from LLM.</li> </ul>"},{"location":"CHANGELOG/#0151","title":"0.15.1","text":"<ul> <li>Fix mcp tool calls.</li> <li>Improve eca filesystem calls for better tool usage from LLM.</li> <li>Fix default model selection to check anthropic api key before.</li> </ul>"},{"location":"CHANGELOG/#0150","title":"0.15.0","text":"<ul> <li>Support MCP resources as a new context.</li> </ul>"},{"location":"CHANGELOG/#0144","title":"0.14.4","text":"<ul> <li>Fix usage miscalculation.</li> </ul>"},{"location":"CHANGELOG/#0143","title":"0.14.3","text":"<ul> <li>Fix reason-id on openai models afecting chat thoughts messages.</li> <li>Support openai o models reason text when available.</li> </ul>"},{"location":"CHANGELOG/#0142","title":"0.14.2","text":"<ul> <li>Fix MCPs not starting because of graal reflection issue.</li> </ul>"},{"location":"CHANGELOG/#0141","title":"0.14.1","text":"<ul> <li>Fix native image build.</li> </ul>"},{"location":"CHANGELOG/#0140","title":"0.14.0","text":"<ul> <li>Support enable/disable tool servers.</li> <li>Bump mcp java sdk to 0.11.0.</li> </ul>"},{"location":"CHANGELOG/#0131","title":"0.13.1","text":"<ul> <li>Improve ollama model listing getting capabilities, avoiding change ollama config for different models.</li> </ul>"},{"location":"CHANGELOG/#0130","title":"0.13.0","text":"<ul> <li>Support reasoning for ollama models that support think.</li> </ul>"},{"location":"CHANGELOG/#0127","title":"0.12.7","text":"<ul> <li>Fix ollama tool calls.</li> </ul>"},{"location":"CHANGELOG/#0126","title":"0.12.6","text":"<ul> <li>fix web-search support for custom providers.</li> <li>fix output of eca_shell_command.</li> </ul>"},{"location":"CHANGELOG/#0125","title":"0.12.5","text":"<ul> <li>Improve tool call result marking as error when not expected output.</li> <li>Fix cases when tool calls output nothing.</li> </ul>"},{"location":"CHANGELOG/#0124","title":"0.12.4","text":"<ul> <li>Add chat command type.</li> </ul>"},{"location":"CHANGELOG/#0123","title":"0.12.3","text":"<ul> <li>Fix MCP prompts for anthropic models.</li> </ul>"},{"location":"CHANGELOG/#0122","title":"0.12.2","text":"<ul> <li>Fix tool calls</li> </ul>"},{"location":"CHANGELOG/#0121","title":"0.12.1","text":"<ul> <li>Improve welcome message.</li> </ul>"},{"location":"CHANGELOG/#0120","title":"0.12.0","text":"<ul> <li>Fix openai api key read from config.</li> <li>Support commands via <code>/</code>.</li> <li>Support MCP prompts via commands.</li> </ul>"},{"location":"CHANGELOG/#0112","title":"0.11.2","text":"<ul> <li>Fix error field on tool call outputs.</li> </ul>"},{"location":"CHANGELOG/#0111","title":"0.11.1","text":"<ul> <li>Fix reasoning for openai o models.</li> </ul>"},{"location":"CHANGELOG/#0110","title":"0.11.0","text":"<ul> <li>Add support for file contexts with line ranges.</li> </ul>"},{"location":"CHANGELOG/#0103","title":"0.10.3","text":"<ul> <li>Fix openai <code>max_output_tokens</code> message.</li> </ul>"},{"location":"CHANGELOG/#0102","title":"0.10.2","text":"<ul> <li>Fix usage metrics for anthropic models.</li> </ul>"},{"location":"CHANGELOG/#0101","title":"0.10.1","text":"<ul> <li>Improve <code>eca_read_file</code> tool to have better and more assertive descriptions/parameters.</li> </ul>"},{"location":"CHANGELOG/#0100","title":"0.10.0","text":"<ul> <li>Increase anthropic models maxTokens to 8196</li> <li>Support thinking/reasoning on models that support it.</li> </ul>"},{"location":"CHANGELOG/#090","title":"0.9.0","text":"<ul> <li>Include eca as a  server with tools.</li> <li>Support disable tools via config.</li> <li>Improve ECA prompt to be more precise and output with better quality</li> </ul>"},{"location":"CHANGELOG/#081","title":"0.8.1","text":"<ul> <li>Make generic tool server updates for eca native tools.</li> </ul>"},{"location":"CHANGELOG/#080","title":"0.8.0","text":"<ul> <li>Support tool call approval and configuration to manual approval.</li> <li>Initial support for repo-map context.</li> </ul>"},{"location":"CHANGELOG/#070","title":"0.7.0","text":"<ul> <li>Add client request to delete a chat.</li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li>Support defaultModel in custom providers.</li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>Add usage tokens + cost to chat messages.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>Fix openai key</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>Support custom LLM providers via config.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li>Improve context query performance.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>Fix output of errored tool calls.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li>Fix arguments test when preparing tool call.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>Add support for global rules.</li> <li>Fix origin field of tool calls.</li> <li>Allow chat communication with no workspace opened.</li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Improve default model logic to check for configs and env vars of known models.</li> <li>Fix past messages sent to LLMs.</li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Support stop chat prompts via <code>chat/promptStop</code> notification.</li> <li>Fix anthropic messages history.</li> </ul>"},{"location":"CHANGELOG/#020","title":"0.2.0","text":"<ul> <li>Add native tools: filesystem</li> <li>Add MCP/tool support for ollama models.</li> <li>Improve ollama integration only requiring <code>ollama serve</code> to be running.</li> <li>Improve chat history and context passed to all LLM providers.</li> <li>Add support for prompt caching for Anthropic models.</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Allow comments on <code>json</code> configs.</li> <li>Improve MCP tool call feedback.</li> <li>Add support for env vars in mcp configs.</li> <li>Add <code>mcp/serverUpdated</code> server notification.</li> </ul>"},{"location":"CHANGELOG/#004","title":"0.0.4","text":"<ul> <li>Add env support for MCPs</li> <li>Add web_search capability</li> <li>Add <code>o3</code> model support.</li> <li>Support custom API urls for OpenAI and Anthropic</li> <li>Add <code>--log-level &lt;level&gt;</code> option for better debugging.</li> <li>Add support for global config file.</li> <li>Improve MCP response handling.</li> <li>Improve LLM streaming response handler.</li> </ul>"},{"location":"CHANGELOG/#003","title":"0.0.3","text":"<ul> <li>Fix ollama servers discovery</li> <li>Fix <code>.eca/config.json</code> read from workspace root</li> <li>Add support for MCP servers</li> </ul>"},{"location":"CHANGELOG/#002","title":"0.0.2","text":"<ul> <li>First alpha release</li> </ul>"},{"location":"CHANGELOG/#001","title":"0.0.1","text":""},{"location":"configuration/","title":"Configuration","text":"<p>Check all available configs and its default values here.</p>"},{"location":"configuration/#ways-to-configure","title":"Ways to configure","text":"<p>There are multiples ways to configure ECA:</p> Global config fileLocal Config fileInitializationOptionsEnv var <p>Convenient for users and multiple projects</p> <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre></p> <p>Convenient for users</p> <p><code>.eca/config.json</code> <pre><code>{\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre></p> <p>Convenient for editors</p> <p>Client editors can pass custom settings when sending the <code>initialize</code> request via the <code>initializationOptions</code> object:</p> <pre><code>\"initializationOptions\": {\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre> <p>Via env var during server process spawn:</p> <pre><code>ECA_CONFIG='{\"myConfig\": \"my_value\"}' eca server\n</code></pre>"},{"location":"configuration/#providers-models","title":"Providers / Models","text":"<p>For providers and models configuration check the dedicated models section.</p>"},{"location":"configuration/#tools","title":"Tools","text":""},{"location":"configuration/#mcp","title":"MCP","text":"<p>For MCP servers configuration, use the <code>mcpServers</code> config, examples:</p> StdioHTTP-streamable <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"],\n      // optional\n      \"env\": {\"FOO\": \"bar\"}\n    }\n  }\n}\n</code></pre></p> <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"mcpServers\": {\n    \"cool-mcp\": {\n      \"url\": \"https://my-remote-mcp.com/mcp\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"configuration/#approval-permissions","title":"Approval / permissions","text":"<p>By default, ECA ask to call any tool, but that's can easily be configureed in many ways via the <code>toolCall approval</code> config.</p> <p>You can configure the default behavior via <code>byDefault</code> and/or configure a tool in <code>ask</code>, <code>allow</code> or <code>deny</code> configs.</p> <p>Check some examples:</p> Allow any tools by defaultAllow all but some toolsAsk all but all tools from some mcpsMatching by a tool argumentDenying a tool <pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\"\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"ask\": {\n        \"eca_editfile\": {},\n        \"my-mcp__my_tool\": {}\n      }\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      // \"byDefault\": \"ask\", not needed as it's eca default\n      \"allow\": {\n        \"eca\": {},\n        \"my-mcp\": {}\n      }\n    }\n  }\n}\n</code></pre> <p><code>argsMatchers</code> is a map of argument name by list of java regex.</p> <pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"ask\": {\n        \"eca_shell_command\": {\"argsMatchers\": {\"command\": [\".*rm.*\",\n                                                           \".*mv.*\"]}}\n      }\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"deny\": {\n        \"eca_shell_command\": {\"argsMatchers\": {\"command\": [\".*rm.*\",\n                                                           \".*mv.*\"]}}\n      }\n    }\n  }\n}\n</code></pre> <p>Also check the <code>plan</code> behavior which is safer.</p> <p>The <code>manualApproval</code> setting was deprecated and replaced by the <code>approval</code> one without breaking changes</p>"},{"location":"configuration/#file-reading","title":"File Reading","text":"<p>You can configure the maximum number of lines returned by the <code>eca_read_file</code> tool:</p> <pre><code>{\n  \"toolCall\": {\n    \"readFile\": {\n      \"maxLines\": 1000\n    }\n  }\n}\n</code></pre> <p>Default: <code>2000</code> lines</p>"},{"location":"configuration/#custom-tools","title":"Custom Tools","text":"<p>You can define your own command-line tools that the LLM can use. These are configured via the <code>customTools</code> key in your <code>config.json</code>.</p> <p>The <code>customTools</code> value is an object where each key is the name of your tool. Each tool definition has the following properties:</p> <ul> <li><code>description</code>: A clear description of what the tool does. This is crucial for the LLM to decide when to use it.</li> <li><code>command</code>: An string representing the command and its static arguments.</li> <li><code>schema</code>: An object that defines the parameters the LLM can provide.<ul> <li><code>properties</code>: An object where each key is an argument name.</li> <li><code>required</code>: An array of required argument names.</li> </ul> </li> </ul> <p>Placeholders in the format <code>{{argument_name}}</code> within the <code>command</code> string will be replaced by the values provided by the LLM.</p> Example 1Example 2 <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"customTools\": {\n    \"web-search\": {\n      \"description\": \"Fetches the content of a URL and returns it in Markdown format.\",\n      \"command\": \"trafilatura --output-format=markdown -u {{url}}\",\n      \"schema\": {\n        \"properties\": {\n          \"url\": {\n            \"type\": \"string\",\n            \"description\": \"The URL to fetch content from.\"\n          }\n        },\n        \"required\": [\"url\"]\n      }\n    }\n  }\n}\n</code></pre></p> <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"customTools\": {\n    \"file-search\": {\n      \"description\": \"Finds files within a directory that match a specific name pattern.\",\n      \"command\": \"find {{directory}} -name {{pattern}}\",\n      \"schema\": {\n        \"properties\": {\n          \"directory\": {\n            \"type\": \"string\",\n            \"description\": \"The directory to start the search from.\"\n          },\n          \"pattern\": {\n            \"type\": \"string\",\n            \"description\": \"The search pattern for the filename (e.g., '*.clj').\"\n          }\n        },\n        \"required\": [\"directory\", \"pattern\"]\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"configuration/#custom-command-prompts","title":"Custom command prompts","text":"<p>You can configure custom command prompts for project, global or via <code>commands</code> config pointing to the path of the commands. Prompts can use variables like <code>$ARGUMENTS</code>, <code>$ARG1</code>, <code>ARG2</code>, to replace in the prompt during command call.</p> Local custom commandsGlobal custom commandsConfig <p>A <code>.eca/commands</code> folder from the workspace root containing <code>.md</code> files with the custom prompt.</p> <p><code>.eca/commands/check-performance.md</code> <pre><code>Check for performance issues in $ARG1 and optimize if needed.\n</code></pre></p> <p>A <code>$XDG_CONFIG_HOME/eca/commands</code> or <code>~/.config/eca/commands</code> folder containing <code>.md</code> files with the custom command prompt.</p> <p><code>~/.config/eca/commands/check-performance.md</code> <pre><code>Check for performance issues in $ARG1 and optimize if needed.\n</code></pre></p> <p>Just add to your config the <code>commands</code> pointing to <code>.md</code> files that will be searched from the workspace root if not an absolute path:</p> <pre><code>{\n  \"commands\": [{\"path\": \"my-custom-prompt.md\"}]\n}\n</code></pre>"},{"location":"configuration/#rules","title":"Rules","text":"<p>Rules are contexts that are passed to the LLM during a prompt and are useful to tune prompts or LLM behavior. Rules are text files (typically <code>.md</code>, but any format works) with the following optional metadata:</p> <ul> <li><code>description</code>: a description used by LLM to decide whether to include this rule in context, absent means always include this rule.</li> <li><code>globs</code>: list of globs separated by <code>,</code>. When present the rule will be applied only when files mentioned matches those globs.</li> </ul> <p>There are 3 possible ways to configure rules following this order of priority:</p> Project fileGlobal fileConfig <p>A <code>.eca/rules</code> folder from the workspace root containing <code>.md</code> files with the rules.</p> <p><code>.eca/rules/talk_funny.md</code> <pre><code>---\ndescription: Use when responding anything\n---\n\n- Talk funny like Mickey!\n</code></pre></p> <p>A <code>$XDG_CONFIG_HOME/eca/rules</code> or <code>~/.config/eca/rules</code> folder containing <code>.md</code> files with the rules.</p> <p><code>~/.config/eca/rules/talk_funny.md</code> <pre><code>---\ndescription: Use when responding anything\n---\n\n- Talk funny like Mickey!\n</code></pre></p> <p>Just add toyour config the <code>:rules</code> pointing to <code>.md</code> files that will be searched from the workspace root if not an absolute path:</p> <pre><code>{\n  \"rules\": [{\"path\": \"my-rule.md\"}]\n}\n</code></pre>"},{"location":"configuration/#behaviors-prompts","title":"Behaviors / prompts","text":"<p>ECA allows to totally customize the prompt sent to LLM via the <code>behavior</code> config, allowing to have multiple behaviors for different tasks or workflows.</p> Example: my-behavior <pre><code>{\n  \"behavior\": {\n    \"my-behavior\": {\n      \"systemPromptFile\": \"/path/to/my-behavior-prompt.md\"\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#opentelemetry-integration","title":"Opentelemetry integration","text":"<p>To configure, add your OTLP collector config via <code>:otlp</code> map following otlp auto-configure settings. Example:</p> <pre><code>{\n  \"otlp\": {\n    \"otel.exporter.otlp.metrics.protocol\": \"http/protobuf\",\n    \"otel.exporter.otlp.metrics.endpoint\": \"https://my-otlp-endpoint.com/foo\",\n    \"otel.exporter.otlp.headers\": \"Authorization=Bearer 123456\"\n  }\n}\n</code></pre>"},{"location":"configuration/#all-configs","title":"All configs","text":"SchemaDefault values <pre><code>interface Config {\n    providers?: {[key: string]: {\n        api?: 'openai-responses' | 'openai-chat' | 'anthropic';\n        url?: string;\n        urlEnv?: string;\n        key?: string; // when provider supports api key.\n        keyEnv?: string;\n        keyRc?: string; // credential file lookup in format [login@]machine[:port]\n        completionUrlRelativePath?: string;\n        models: {[key: string]: {\n          extraPayload?: {[key: string]: any}\n        }};\n    }};\n    defaultModel?: string;\n    rules?: [{path: string;}];\n    commands?: [{path: string;}];\n    behavior?: {[key: string]: {\n        systemPromptFile?: string;\n        defaultModel?: string;\n        disabledTools?: string[];\n        toolCall?: {\n            approval?: {\n                byDefault?: 'ask' | 'allow' | 'deny';\n                allow?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n                ask?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n                deny?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n            };\n        };\n    }};\n    customTools?: {[key: string]: {\n        description: string;\n        command: string;\n        schema: {\n            properties: {[key: string]: {\n                type: string;\n                description: string;\n            }};\n            required: string[];\n        };\n    }};\n    disabledTools?: string[],\n    toolCall?: {\n      approval?: {\n        byDefault: 'ask' | 'allow';\n        allow?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n        ask?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n        deny?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n      };\n      readFile?: {\n        maxLines?: number;\n      };\n      shellCommand?: {\n        summaryMaxLength?: number,\n      };\n    };\n    mcpTimeoutSeconds?: number;\n    lspTimeoutSeconds?: number;\n    mcpServers?: {[key: string]: {\n        url?: string; // for remote http-stremable servers\n        command?: string; // for stdio servers\n        args?: string[];\n        disabled?: boolean;\n    }};\n    defaultBehavior?: string;\n    welcomeMessage?: string;\n    compactPromptFile?: string;\n    index?: {\n        ignoreFiles: [{\n            type: string;\n        }];\n        repoMap?: {\n            maxTotalEntries?: number;\n            maxEntriesPerDir?: number;\n        };\n    };\n    otlp?: {[key: string]: string};\n}\n</code></pre> <pre><code>{\n  \"providers\": {\n      \"openai\": {\"url\": \"https://api.openai.com\"},\n      \"anthropic\": {\"url\": \"https://api.anthropic.com\"},\n      \"github-copilot\": {\"url\": \"https://api.githubcopilot.com\"},\n      \"google\": {\"url\": \"https://generativelanguage.googleapis.com/v1beta/openai\"},\n      \"ollama\": {\"url\": \"http://localhost:11434\"}\n  },\n  \"defaultModel\": null, // let ECA decides the default model.\n  \"rules\" : [],\n  \"commands\" : [],\n  \"disabledTools\": [],\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"ask\",\n      \"allow\": {\"eca_directory_tree\": {},\n                \"eca_read_file\": {},\n                \"eca_grep\": {},\n                \"eca_preview_file_change\": {},\n                \"eca_editor_diagnostics\": {}},\n      \"ask\": {},\n      \"deny\": {}\n    },\n    \"readFile\": {\n      \"maxLines\": 2000\n    },\n    \"shellCommand\": {\n      \"summaryMaxLength\": 30,\n    },\n  },\n  \"mcpTimeoutSeconds\" : 60,\n  \"lspTimeoutSeconds\" : 30,\n  \"mcpServers\" : {},\n  \"behavior\" {\n    \"agent\": {\"systemPromptFile\": \"prompts/agent_behavior.md\",\n              \"disabledTools\": [\"eca_preview_file_change\"]},\n    \"plan\": {\"systemPromptFile\": \"prompts/plan_behavior.md\",\n              \"disabledTools\": [\"eca_edit_file\", \"eca_write_file\", \"eca_move_file\"],\n              \"toolCall\": {\"approval\": {\"deny\": {\"eca_shell_command\":\n                                                 {\"argsMatchers\": {\"command\" [\".*&gt;.*\",\n                                                                              \".*\\\\|\\\\s*(tee|dd|xargs).*\",\n                                                                              \".*\\\\b(sed|awk|perl)\\\\s+.*-i.*\",\n                                                                              \".*\\\\b(rm|mv|cp|touch|mkdir)\\\\b.*\",\n                                                                              \".*git\\\\s+(add|commit|push).*\",\n                                                                              \".*npm\\\\s+install.*\",\n                                                                              \".*-c\\\\s+[\\\"'].*open.*[\\\"']w[\\\"'].*\",\n                                                                              \".*bash.*-c.*&gt;.*\"]}}}}}}\n  }\n  \"defaultBehavior\": \"agent\",\n  \"welcomeMessage\" : \"Welcome to ECA!\\n\\nType '/' for commands\\n\\n\",\n  \"compactPromptFile\": \"prompts/compact.md\",\n  \"index\" : {\n    \"ignoreFiles\" : [ {\n      \"type\" : \"gitignore\"\n    } ],\n    \"repoMap\": {\n      \"maxTotalEntries\": 800,\n      \"maxEntriesPerDir\": 50\n    }\n  }\n}\n</code></pre>"},{"location":"development/","title":"ECA Development","text":""},{"location":"development/#building-local","title":"Building local","text":"<ul> <li>Install babashka.</li> <li>Run <code>bb debug-cli</code>, it will geneate a JVM embeeded binary at project root where yuo can just <code>./eca</code>.</li> </ul>"},{"location":"development/#project-structure","title":"Project structure","text":"<p>The ECA codebase follows a pragmatic layered layout that separates concerns clearly so that you can jump straight to the part you need to change.</p>"},{"location":"development/#files-overview","title":"Files overview","text":"Path Responsibility <code>bb.edn</code> Babashka tasks (e.g. <code>bb test</code>, <code>bb debug-cli</code>) for local workflows and CI, the main entrypoint for most tasks. <code>deps.edn</code> Clojure dependency coordinates and aliases used by the JVM build and the native GraalVM image. <code>docs/</code> Markdown documentation shown at https://eca.dev <code>src/eca/config.clj</code> Centralized place to get ECA configs from multiple places. <code>src/eca/logger.clj</code> Logger interface to log to stderr. <code>src/eca/shared.clj</code> shared utility fns to whole project. <code>src/eca/db.clj</code> Simple in-memory KV store that backs sessions/MCP, all in-memory statue lives here. <code>src/eca/llm_api.clj</code> Public fa\u00e7ade used by features to call an LLM. <code>src/eca/llm_providers/</code> Vendor adapters (<code>openai.clj</code>, <code>anthropic.clj</code>, <code>ollama.clj</code>). <code>src/eca/llm_util.clj</code> Token counting, chunking, rate-limit helpers. <code>src/eca/features/</code> High-level capabilities exposed to the editor \u251c\u2500 <code>chat.clj</code> Streaming chat orchestration &amp; tool-call pipeline. \u251c\u2500 <code>prompt.clj</code> Prompt templates and variable interpolation. \u251c\u2500 <code>index.clj</code> Embedding &amp; retrieval-augmented generation helpers. \u251c\u2500 <code>rules.clj</code> Guards that enforce user-defined project rules. \u251c\u2500 <code>tools.clj</code> Registry of built-in tool descriptors (run, approve\u2026). \u2514\u2500 <code>tools/</code> Implementation of side-effectful tools: \u2500\u2500\u251c\u2500 <code>filesystem.clj</code> read/write/edit helpers\u2003 \u2500\u2500\u251c\u2500 <code>shell.clj</code> runs user-approved shell commands\u2003 \u2500\u2500\u251c\u2500 <code>mcp.clj</code> Multi-Command Plan supervisor\u2003 \u2500\u2500\u2514\u2500 <code>util.clj</code> misc helpers shared by tools. <code>src/eca/messenger.clj</code> To send back to client requests/notifications over stdio. <code>src/eca/handlers.clj</code> Entrypoint for all features. <code>src/eca/server.clj</code> stdio entry point; wires everything together via <code>jsonrpc4clj</code>. <code>src/eca/main.clj</code> The CLI interface. <code>src/eca/nrepl.clj</code> Starts an nREPL when <code>:debug</code> flag is passed. <p>Together these files implement the request flow: </p> <p><code>client/editor</code> \u2192 <code>stdin JSON-RPC</code> \u2192 <code>handlers</code> \u2192 <code>features</code> \u2192 <code>llm_api</code> \u2192 <code>llm_provider</code> \u2192 results streamed back.</p> <p>With this map you can usually answer:</p> <ul> <li>\"Where does request X enter the system?\" \u2013 look in <code>handlers.clj</code>.</li> <li>\"How is tool Y executed?\" \u2013 see <code>src/eca/features/tools/&lt;y&gt;.clj</code>.</li> <li>\"How do we talk to provider Z?\" \u2013 adapter under <code>llm_providers/</code>.</li> </ul>"},{"location":"development/#unit-tests","title":"Unit Tests","text":"<p>Run with <code>bb test</code> or run test via Clojure REPL. CI will run the same task.</p>"},{"location":"development/#integration-tests","title":"Integration Tests","text":"<p>Run with <code>bb integration-test</code>, it will use your <code>eca</code> binary project root to spawn a server process and communicate with it via JSONRPC, testing the whole eca flow like an editor.</p>"},{"location":"development/#coding","title":"Coding","text":"<p>There are several ways of finding and fixing a bug or implementing a new feature:</p> <ul> <li>Create a test for your bug/feature, then implement the code following the test (TDD).</li> <li>Build a local <code>eca</code> JVM embedded binary using <code>bb debug-cli</code> (requires <code>babashka</code>), and test it manually in your client pointing to it. After started, you can connect to the nrepl port mentioned in eca stderr, do you changes, evaluate and it will be affected on the running eca.</li> <li>Using a debug binary you can connect to the REPL, make changes to the running eca process (really handy).</li> </ul>"},{"location":"development/#supporting-a-new-editor","title":"Supporting a new editor","text":"<p>When supporting a new editor, it's important to keep UX consistency across editors, check how other editors done or ask for help.</p> <p>This step-by-step feature implementation help track progress and next steps:</p> <pre><code>- [ ] Create the plugin/extension repo (editor-code-assistant/eca-&lt;editor&gt; would be ideal), ask maintainers for permission.\n- Server\n  - Manage ECA server process.\n    - [ ] Automatic download of latest server.\n    - [ ] Allow user specify server path/args.\n    - [ ] Commands for Start/stop server from editor.\n    - [ ] Show server status (modeline, bottom of editor, etc).\n  - [ ] JSONRPC communication with eca server process via stdin/stdout sending/receiving requests and notifications, check [protocol](./protocol.md).\n  - [ ] Allow check eca server process stderr for debugging/logs.\n  - [ ] Support `initialize` and `initialized` methods.\n  - [ ] Support `exit` and `shutdown` methods.\n- Chat\n  - [ ] Oepn chat window\n  - [ ] Send user messages via `chat/prompt` request.\n  - [ ] Clear chat and Reset chat.\n  - [ ] Support receive chat contents via `chat/contentReceived` notification.\n  - [ ] Present and allow user change behaviors and models returned from `initialize` request.\n  - [ ] Present and add contexts via `chat/queryContext` request\n  - [ ] Support tools contents: run/approval/reject via `chat/toolCallApprove` or `chat/toolCallReject`.\n  - [ ] Support tools details: showing a file change like a diff.\n  - [ ] Support reason/thoughts content blocks.\n  - [ ] Show MCPs summary (running, failed, pending).\n  - [ ] Support chat commands (`/`) auto completion, querying via `chat/queryCommands`.\n  - [ ] Show usage (costs/tokens) from usage content blocks.\n  - [ ] keybindings: navigate through chat blocks/messages, clear chat.\n- MCP\n  - [ ] Open MCP details window\n  - [ ] Receive MCP server updates and update chat and mcp-details ux.\n- [ ] Basic plugin/extension documentation\n</code></pre> <p>Create a issue to help track the effort copying and pasting these check box to help track progress, example.</p> <p>Please provide feedback of the dificulties implementing your server, especially missing docs, to make next integrations smoother!</p>"},{"location":"features/","title":"Features","text":""},{"location":"features/#chat","title":"Chat","text":"<p>Chat is the main feature of ECA, allowing user to talk with LLM to behave like an agent, making changes using tools or just planning changes and next steps.</p>"},{"location":"features/#behaviors","title":"Behaviors","text":"<p>Behavior affect the prompt passed to LLM and the tools to include, ECA allow to override or create your owns behaviors, the built-in provider behaviors are:</p> <ul> <li><code>plan</code>: Useful to plan changes and define better LLM plan before changing code via agent mode, has ability to preview changes (Check picture). Prompt here</li> <li><code>agent</code>: Make changes to code via file changing tools. (Default) Prompt here</li> </ul> <p></p> <p>To create and customize your own behaviors, check config.</p>"},{"location":"features/#tools","title":"Tools","text":"<p>ECA leverage tools to give more power to the LLM, this is the best way to make LLMs have more context about your codebase and behave like an agent. It supports both MCP server tools + ECA native tools.</p> <p>Approval / permissions</p> <p>By default, ECA ask to approve any tool, you can easily configure that, check <code>toolCall approval</code> config or try the <code>plan</code> behavior.</p>"},{"location":"features/#native-tools","title":"Native tools","text":"<p>ECA support built-in tools to avoid user extra installation and configuration, these tools are always included on models requests that support tools and can be disabled via config <code>disabledTools</code>.</p>"},{"location":"features/#filesystem","title":"Filesystem","text":"<p>Provides access to filesystem under workspace root, listing, reading and writing files, important for agentic operations.</p> <ul> <li><code>eca_directory_tree</code>: list a directory as a tree (can be recursive).</li> <li><code>eca_read_file</code>: read a file content.</li> <li><code>eca_write_file</code>: write content to a new file.</li> <li><code>eca_edit_file</code>: replace lines of a file with a new content.</li> <li><code>eca_preview_edit_file</code>: Only used in plan mode, showing what changes will happen after user decides to execute the plan.</li> <li><code>eca_move_file</code>: move/rename a file.</li> <li><code>eca_grep</code>: ripgrep/grep for paths with specified content.</li> </ul>"},{"location":"features/#shell","title":"Shell","text":"<p>Provides access to run shell commands, useful to run build tools, tests, and other common commands, supports exclude/include commands. </p> <ul> <li><code>eca_shell_command</code>: run shell command. Command exclusion can be configured using toolCall approval configuration with regex patterns.</li> </ul>"},{"location":"features/#editor","title":"Editor","text":"<p>Provides access to get information from editor workspaces.</p> <ul> <li><code>eca_editor_diagnostics</code>: Ask client about the diagnostics (like LSP diagnostics).</li> </ul>"},{"location":"features/#custom-tools","title":"Custom Tools","text":"<p>Besides the built-in native tools, ECA allows you to define your own tools by wrapping any command-line executable. This feature enables you to extend ECA's capabilities to match your specific workflows, such as running custom scripts, interacting with internal services, or using your favorite CLI tools.</p> <p>Custom tools are configured in your <code>config.json</code> file. For a detailed guide on how to set them up, check the Custom Tools configuration documentation.</p>"},{"location":"features/#contexts","title":"Contexts","text":"<p>User can include contexts to the chat (<code>@</code>), including images and MCP resources, which can help LLM generate output with better quality. Here are the current supported contexts types:</p> <ul> <li><code>file</code>: a file in the workspace, server will pass its content to LLM (Supports optional line range) and images.</li> <li><code>directory</code>: a directory in the workspace, server will read all file contexts and pass to LLM.</li> <li>~<code>repoMap</code>: a summary view of workspaces files and folders, server will calculate this and pass to LLM. Currently, the repo-map includes only the file paths in git.~</li> <li><code>cursor</code>: Current file path + cursor position or selection.</li> <li><code>mcpResource</code>: resources provided by running MCPs servers.</li> </ul>"},{"location":"features/#agentsmd-automatic-context","title":"AGENTS.md automatic context","text":"<p>ECA will always include if found the <code>AGENTS.md</code> file as context, searching for both <code>/project-root/AGENTS.md</code> and <code>~/.config/eca/AGENTS.md</code>.</p> <p>You can ask ECA to create/update this file via <code>/init</code> command.</p>"},{"location":"features/#commands","title":"Commands","text":"<p>Eca supports commands that usually are triggered via shash (<code>/</code>) in the chat, completing in the chat will show the known commands which include ECA commands, MCP prompts and resources.</p> <p>The built-in commands are:</p> <p><code>/init</code>: Create/update the AGENTS.md file with details about the workspace for best LLM output quality. <code>/login</code>: Log into a provider. Ex: <code>github-copilot</code>, <code>anthropic</code>. <code>/compact</code>: Compact/summarize conversation helping reduce context window. <code>/resume</code>: Resume a chat from previous session of this workspace folder. <code>/costs</code>: Show costs about current session. <code>/config</code>: Show ECA config for troubleshooting. <code>/doctor</code>: Show information about ECA, useful for troubleshooting. <code>/repo-map-show</code>: Show the current repoMap context of the session. <code>/prompt-show</code>: Show the final prompt sent to LLM with all contexts and ECA details.</p>"},{"location":"features/#custom-commands","title":"Custom commands","text":"<p>It's possible to configure custom command prompts, for more details check its configuration</p>"},{"location":"features/#login","title":"Login","text":"<p>It's possible to login to some providers using <code>/login</code> command, ECA will ask and give instructions on how to authenticate in the chosen provider and save the login info globally in its cache <code>~/.cache/eca/db.transit.json</code>.</p> <p>Current supported providers with login: - <code>anthropic</code>: with options to login to Claude Max/Pro or create API keys. - <code>github-copilot</code>: via Github oauth.</p>"},{"location":"features/#opentelemetry-integration","title":"OpenTelemetry integration","text":"<p>ECA has support for OpenTelemetry(otlp), if configured, server tasks, tool calls, and more will be metrified via otlp API.</p> <p>For more details check its configuration</p>"},{"location":"features/#completion","title":"Completion","text":"<p>Soon</p>"},{"location":"features/#edit","title":"Edit","text":"<p>Soon</p>"},{"location":"installation/","title":"Installation","text":"<p>Eca is written in Clojure and compiled into a native binary via graalvm.</p> <p>Warning</p> <p>ECA is already automatically downloaded and updated in all editor plugins, so you don't need to handle it manually, even so, if you want that, check the other methods.</p> Editor (recommended)Script (recommended if manual installing)HomebrewmiseGtihub releases <p>ECA is already downloaded automatically by your ECA editor plugin, so you just need to install the plugin for your editor:</p> <ul> <li>Emacs</li> <li>VsCode</li> <li>Vim</li> <li>Intellij</li> </ul> <p>Stable release:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install)\n</code></pre> <p>Or if facing issues with command above: <pre><code>curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install | sudo bash\n</code></pre></p> <p>nightly build:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install) --version nightly --dir ~/\n</code></pre> <p>We have a custom tap using the native compiled binaries for users that use homebrew:</p> <pre><code>brew install editor-code-assistant/brew/eca\n</code></pre> <p>Install using mise </p> <pre><code># Install the plugin\nmise plugin install eca https://github.com/editor-code-assistant/eca-mise-plugin\n\n# Install latest version ECA\nmise install eca\nmise use -g eca\n\n# or install and use\n# desired version\nmise install eca@0.58.0\nmise use -g eca@0.58.0\n\n# Verify installation\neca --version\n</code></pre> <p>You can download the native binaries from Github Releases, although it's easy to have outdated ECA using this way.</p>"},{"location":"models/","title":"Models","text":"<p>Info</p> <p>Most providers can be configured via <code>/login</code> command, otherwise via <code>providers</code> config.</p> <p>Models capabilities and configurations are retrieved from models.dev API.</p> <p>ECA will return to clients the models configured, either via config or login.</p>"},{"location":"models/#built-in-providers-and-capabilities","title":"Built-in providers and capabilities","text":"model tools (MCP) reasoning / thinking prompt caching web_search image_input OpenAI \u221a \u221a \u221a \u221a \u221a Anthropic (Also subscription) \u221a \u221a \u221a \u221a \u221a Github Copilot \u221a \u221a \u221a X \u221a Google \u221a \u221a \u221a X \u221a Ollama local models \u221a \u221a X X"},{"location":"models/#config","title":"Config","text":"<p>Built-in providers have already base initial <code>providers</code> configs, so you can change to add models or set its key/url.</p> <p>For more details, check the config schema.</p> <p>Example:</p> <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"providers\": {\n    \"openai\": {\n      \"key\": \"your-openai-key-here\", // configuring a key\n      \"models\": {\n        \"o1\": {} // adding models to a built-in provider\n        \"o3\": {\n          \"extraPayload\": { // adding to the payload sent to LLM\n            \"temperature\": 0.5\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre></p> <p>Environment Variables: You can also set API keys using environment variables following <code>\"&lt;PROVIDER&gt;_API_KEY\"</code>, examples:</p> <ul> <li><code>OPENAI_API_KEY</code> for OpenAI</li> <li><code>ANTHROPIC_API_KEY</code> for Anthropic</li> </ul>"},{"location":"models/#custom-providers","title":"Custom providers","text":"<p>ECA allows you to configure custom LLM providers that follow API schemas similar to OpenAI or Anthropic. This is useful when you want to use:</p> <ul> <li>Self-hosted LLM servers (like LiteLLM)</li> <li>Custom company LLM endpoints</li> <li>Additional cloud providers not natively supported</li> </ul> <p>You just need to add your provider to <code>providers</code> and make sure add the required fields</p> <p>Schema:</p> Option Type Description Required <code>api</code> string The API schema to use (<code>\"openai-responses\"</code>, <code>\"openai-chat\"</code>, or <code>\"anthropic\"</code>) Yes <code>urlEnv</code> string Environment variable name containing the API URL No* <code>url</code> string Direct API URL (use instead of <code>urlEnv</code>) No* <code>keyEnv</code> string Environment variable name containing the API key No* <code>keyRc</code> string Lookup specification to read the API key from Unix RC credential files No* <code>key</code> string Direct API key (use instead of <code>keyEnv</code>) No* <code>completionUrlRelativePath</code> string Optional override for the completion endpoint path (see defaults below and examples like Azure) No <code>models</code> map Key: model name, value: its config Yes <code>models &lt;model&gt; extraPayload</code> map Extra payload sent in body to LLM No <p>* url and key will be search as env <code>&lt;provider&gt;_API_URL</code> / <code>&lt;provider&gt;_API_KEY</code>, but require config or to be found to work.</p> <p>Example:</p> <p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"providers\": {\n    \"my-company\": {\n      \"api\": \"openai-chat\",\n      \"urlEnv\": \"MY_COMPANY_API_URL\", // or \"url\"\n      \"keyEnv\": \"MY_COMPANY_API_KEY\", // or \"key\"\n      \"models\": {\n        \"gpt-5\": {},\n        \"deepseek-r1\": {}\n       }\n    }\n  }\n}\n</code></pre></p>"},{"location":"models/#api-types","title":"API Types","text":"<p>When configuring custom providers, choose the appropriate API type:</p> <ul> <li><code>anthropic</code>: Anthropic's native API for Claude models.</li> <li><code>openai-responses</code>: OpenAI's new responses API endpoint (<code>/v1/responses</code>). Best for OpenAI models with enhanced features like reasoning and web search.</li> <li><code>openai-chat</code>: Standard OpenAI Chat Completions API (<code>/v1/chat/completions</code>). Use this for most third-party providers:<ul> <li>OpenRouter</li> <li>DeepSeek</li> <li>Together AI</li> <li>Groq</li> <li>Local LiteLLM servers</li> <li>Any OpenAI-compatible provider</li> </ul> </li> </ul> <p>Most third-party providers use the <code>openai-chat</code> API for compatibility with existing tools and libraries.</p>"},{"location":"models/#endpoint-override-completionurlrelativepath","title":"Endpoint override (completionUrlRelativePath)","text":"<p>Some providers require a non-standard or versioned completion endpoint path. Use <code>completionUrlRelativePath</code> to override the default path appended to your provider <code>url</code>.</p> <p>Defaults by API type: - <code>openai-responses</code>: <code>/v1/responses</code> - <code>openai-chat</code>: <code>/v1/chat/completions</code> - <code>anthropic</code>: <code>/v1/messages</code></p> <p>Only set this when your provider uses a different path or expects query parameters at the endpoint (e.g., Azure API versioning).</p>"},{"location":"models/#credential-file-authentication","title":"Credential File Authentication","text":"<p>Use <code>keyRc</code> in your provider config to read credentials from <code>~/.authinfo(.gpg)</code> or <code>~/.netrc(.gpg)</code> without storing keys directly in config or env vars.</p> <p>Example:</p> <pre><code>{\n  \"providers\": {\n    \"openai\": {\"keyRc\": \"api.openai.com\"},\n    \"anthropic\": {\"keyRc\": \"work@api.anthropic.com\"}\n  }\n}\n</code></pre> <p>keyRc lookup specification format: <code>[login@]machine[:port]</code> (e.g., <code>api.openai.com</code>, <code>work@api.anthropic.com</code>, <code>api.custom.com:8443</code>).</p> <p>Further reading on credential file formats: - Emacs authinfo documentation - Curl Netrc documentation - GNU Inetutils .netrc documentation</p> <p>Notes: - Preferred files are GPG-encrypted (<code>~/.authinfo.gpg</code> / <code>~/.netrc.gpg</code>); plaintext variants are supported. - Authentication priority (short): <code>key</code> &gt; <code>keyRc files</code> &gt; <code>keyEnv</code> &gt; OAuth. - All providers with API key auth can use credential files.</p>"},{"location":"models/#providers-examples","title":"Providers examples","text":"AnthropicGithub CopilotGoogle / GeminiLiteLLMOpenRouterDeepSeekAzure OpenAIZ.aiSame model with different settings <ol> <li>Login to Anthropic via the chat command <code>/login</code>.</li> <li>Type 'anthropic' and send it.</li> <li>Type the chosen method</li> <li>Authenticate in your browser, copy the code.</li> <li>Paste and send the code and done!</li> </ol> <ol> <li>Login to Github copilot via the chat command <code>/login</code>.</li> <li>Type 'github-copilot' and send it.</li> <li>Authenticate in Github in your browser with the given code.</li> <li>Type anything in the chat to continue and done!</li> </ol> <p>Tip: check Your Copilot plan to enable models to your account.</p> <ol> <li>Login to Google via the chat command <code>/login</code>.</li> <li>Type 'google' and send it.</li> <li>Choose 'manual' and type your Google/Gemini API key. (You need to create a key in google studio)</li> </ol> <pre><code>{\n  \"providers\": {\n    \"litellm\": {\n      \"api\": \"openai-responses\",\n      \"url\": \"https://litellm.my-company.com\", // or \"urlEnv\"\n      \"key\": \"your-api-key\", // or \"keyEnv\"\n      \"models\": {\n        \"gpt-5\": {},\n        \"deepseek-r1\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>OpenRouter provides access to many models through a unified API:</p> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'openrouter' and send it.</li> <li>Specify your Openrouter API key.</li> <li>Inform at least a model, ex: <code>openai/gpt-5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> <pre><code>{\n  \"providers\": {\n    \"openrouter\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"https://openrouter.ai/api/v1\", // or \"urlEnv\"\n      \"key\": \"your-api-key\", // or \"keyEnv\"\n      \"models\": {\n        \"anthropic/claude-3.5-sonnet\": {},\n        \"openai/gpt-4-turbo\": {},\n        \"meta-llama/llama-3.1-405b\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>DeepSeek offers powerful reasoning and coding models:</p> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'deepseek' and send it.</li> <li>Specify your Deepseek API key.</li> <li>Inform at least a model, ex: <code>deepseek-chat</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> <pre><code>{\n  \"providers\": {\n    \"deepseek\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"https://api.deepseek.com\", // or \"urlEnv\"\n      \"key\": \"your-api-key\", // or \"keyEnv\"\n      \"models\": {\n        \"deepseek-chat\": {},\n        \"deepseek-coder\": {},\n        \"deepseek-reasoner\": {}\n       }\n    }\n  }\n}\n</code></pre> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'azure' and send it.</li> <li>Specify your API key.</li> <li>Specify your API url with your resource, ex: 'https://your-resource-name.openai.azure.com'.</li> <li>Inform at least a model, ex: <code>gpt-5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> <pre><code>{\n  \"providers\": {\n    \"azure\": {\n      \"api\": \"openai-responses\",\n      \"url\": \"https://your-resource-name.openai.azure.com\", // or \"urlEnv\"\n      \"key\": \"your-api-key\", // or \"keyEnv\"\n      \"completionUrlRelativePath\": \"/openai/responses?api-version=2025-04-01-preview\",\n      \"models\": {\n        \"gpt-5\": {}\n       }\n    }\n  }\n}\n</code></pre> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'azure' and send it.</li> <li>Specify your API key.</li> <li>Inform at least a model, ex: <code>GLM-4.5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> <pre><code>{\n  \"providers\": {\n    \"z-ai\": {\n      \"api\": \"anthropic\",\n      \"url\": \"https://api.z.ai/api/anthropic\",\n      \"key\": \"your-api-key\", // or \"keyEnv\"\n      \"models\": {\n        \"GLM-4.5\": {},\n        \"GLM-4.5-Air\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>For now, you can create different providers with same model names to achieve that:</p> <pre><code>{\n \"providers\": {\n   \"openai\": {\n     \"api\": \"openai-responses\",\n     \"models\": { \"gpt-5\": {} }\n   },\n   \"openai-high\": {\n     \"api\": \"openai-responses\",\n     \"url\": \"https://api.openai.com\",\n     \"keyEnv\": \"OPENAI_API_KEY\",\n     \"models\": {\n       \"gpt-5\": {\n         \"extraPayload\": { \"reasoning\": { \"effort\": \"high\" } }\n       }\n     }\n   }\n }\n}\n</code></pre>"},{"location":"protocol/","title":"ECA Protocol","text":"<p>The ECA (Editor Code Assistant) protocol is JSON-RPC 2.0-based protocol heavily insipired by the LSP (Language Server Protocol), that enables communication between multiple code editors/IDEs and ECA process (server), which will interact with multiple LLMs. It follows similar patterns to the LSP but is specifically designed for AI code assistance features.</p> <p>Key characteristics: - Provides a protocol standard so different editors can use the same language to offer AI features. - Supports bidirectional communication (client to server and server to client) - Handles both synchronous requests and asynchronous notifications - Includes built-in support for streaming responses - Provides structured error handling</p>"},{"location":"protocol/#base-protocol","title":"Base Protocol","text":"<p>The base protocol consists of a header and a content part (comparable to HTTP). The header and content part are separated by a <code>\\r\\n</code>.</p>"},{"location":"protocol/#header-part","title":"Header Part","text":"<p>The header part consists of header fields. Each header field is comprised of a name and a value, separated by <code>:</code> (a colon and a space). The structure of header fields conforms to the HTTP semantic. Each header field is terminated by <code>\\r\\n</code>. Considering the last header field and the overall header itself are each terminated with <code>\\r\\n</code>, and that at least one header is mandatory, this means that two <code>\\r\\n</code> sequences always immediately precede the content part of a message.</p> <p>Currently the following header fields are supported:</p> Header Field Name Value Type Description Content-Length number The length of the content part in bytes. This header is required. Content-Type string The mime type of the content part. Defaults to application/vscode-jsonrpc; charset=utf-8 {: .table .table-bordered .table-responsive} <p>The header part is encoded using the 'ascii' encoding. This includes the <code>\\r\\n</code> separating the header and content part.</p>"},{"location":"protocol/#content-part","title":"Content Part","text":"<p>Contains the actual content of the message. The content part of a message uses JSON-RPC 2.0 to describe requests, responses and notifications. The content part is encoded using the charset provided in the Content-Type field. It defaults to <code>utf-8</code>, which is the only encoding supported right now. If a server or client receives a header with a different encoding than <code>utf-8</code> it should respond with an error.</p>"},{"location":"protocol/#example","title":"Example:","text":"<pre><code>Content-Length: ...\\r\\n\n\\r\\n\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"initialize\",\n    \"params\": {\n        ...\n    }\n}\n</code></pre>"},{"location":"protocol/#lifecycle-messages","title":"Lifecycle Messages","text":"<p>The protocol defines a set of lifecycle messages that manage the connection and state between the client (editor) and server (code assistant).</p> Initialization flowShutdown flow <p>Handshake between client and server, including the actions done by server after initialization.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Client / Editor\n    participant S as ECA Server\n    C-&gt;&gt;+S: initialize (request)\n    Note right of S: Save workspace-folders/capabilties\n    S-&gt;&gt;-C: initialize (response)\n    C--)+S: initialized (notification)\n    Note right of S: Sync models: Request models.dev &lt;br/&gt;for models capabilities\n    Note right of S: Notify which models/behaviors are &lt;br/&gt;avaialble and their defaults.\n    S--)C: config/updated (notification)\n    Note right of S: Init MCP servers\n    S--)-C: tool/serverUpdated (notification)</code></pre> <p>Shutdown process between client and server</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Client / Editor\n    participant S as ECA Server\n    C-&gt;&gt;+S: shutdown\n    Note right of S: Finish MCP servers process\n    S-&gt;&gt;-C: shutdown\n    C--)S: exit\n    Note right of S: Server stops its process</code></pre>"},{"location":"protocol/#initialize","title":"Initialize (\u21a9\ufe0f)","text":"<p>The first request sent from client to server. This message: - Establishes the connection - Allows the server to index the project - Enables capability negotiation - Sets up the workspace context</p> <p>Request:</p> <ul> <li>method: <code>initialize</code></li> <li>params: <code>InitializeParams</code> defined as follows:</li> </ul> <pre><code>interface InitializeParams {\n    /**\n     * The process Id of the parent process that started the server. Is null if\n     * the process has not been started by another process. If the parent\n     * process is not alive then the server should exit (see exit notification)\n     * its process.\n     */\n     processId: integer | null;\n\n     /**\n     * Information about the client\n     */\n    clientInfo?: {\n        /**\n         * The name of the client as defined by the client.\n         */\n        name: string;\n\n        /**\n         * The client's version as defined by the client.\n         */\n        version?: string;\n    };\n\n    /**\n     * User provided initialization options.\n     */\n    initializationOptions?: {\n        /*\n         * The chat behavior.\n         */\n         chatBehavior?: ChatBehavior;\n    };\n\n    /**\n     * The capabilities provided by the client (editor or tool)\n     */\n    capabilities: ClientCapabilities;\n\n    /**\n     * The workspace folders configured in the client when the server starts.\n     * If client doesn\u00b4t support multiple projects, it should send a single \n     * workspaceFolder with the project root.\n     */\n    workspaceFolders: WorkspaceFolder[];\n}\n\ninterface WorkspaceFolder {\n    /**\n     * The associated URI for this workspace folder.\n     */\n    uri: string;\n\n    /**\n     * The name of the workspace folder. Used to refer to this folder in the user interface.\n     */\n    name: string;\n}\n\ninterface ClientCapabilities {\n    codeAssistant?: {\n        chat?: boolean;\n\n        /**\n         * Whether client supports provide editor informations to server like\n         * diagnostics, cursor information and others.\n         */\n        editor?: {\n            /**\n             * Whether client supports provide editor diagnostics \n             * information to server (Ex: LSP diagnostics) via `editor/getDiagnostics` \n             * server request.\n             */ \n            diagnostics?: boolean;\n        }\n    }\n}\n\ntype ChatBehavior = 'agent' | 'plan';\n</code></pre> <p>Response:</p> <pre><code>interface InitializeResponse {}\n</code></pre>"},{"location":"protocol/#initialized","title":"Initialized (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server after receiving the initialize response. This message: - Confirms that the client is ready to receive requests - Signals that the server can start sending notifications - Indicates that the workspace is fully loaded</p> <p>Notification:</p> <ul> <li>method: <code>initialized</code></li> <li>params: <code>InitializedParams</code> defined as follows:</li> </ul> <pre><code>interface InitializedParams {}\n</code></pre>"},{"location":"protocol/#shutdown","title":"Shutdown (\u21a9\ufe0f)","text":"<p>A request sent from the client to the server to gracefully shut down the connection. This message: - Allows the server to clean up resources - Ensures all pending operations are completed - Prepares for a clean disconnection</p> <p>Request:</p> <ul> <li>method: <code>shutdown</code></li> <li>params: none</li> </ul> <p>Response:</p> <ul> <li>result: null</li> <li>error: code and message set in case an exception happens during shutdown request.</li> </ul>"},{"location":"protocol/#exit","title":"Exit (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server to terminate the connection. This message: - Should be sent after a shutdown request - Signals the server to exit its process - Ensures all resources are released</p> <p>Notification:</p> <ul> <li>method: <code>exit</code></li> <li>params: none </li> </ul>"},{"location":"protocol/#code-assistant-features","title":"Code Assistant Features","text":"Chat: textChat: tool call <p>Example of a basic chat conversation with only texts:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Client / Editor\n    participant S as ECA Server\n    participant L as LLM\n    C-&gt;&gt;+S: chat/prompt\n    Note over C,S: User sends: Hello there!\n    S--)C: chat/contentReceived (system: start)\n    S--)C: chat/contentReceived (user: \"hello there!\")\n    Note right of S: Prepare prompt with all&lt;br/&gt;available contexts and tools.\n    S-&gt;&gt;+L: Send prompt\n    S-&gt;&gt;-C: chat/prompt\n    Note over C,S: Success: sent to LLM\n    loop LLM streaming\n        Note right of L: Returns first `H`,&lt;br/&gt;then `i!`, etc\n        L--)S: Stream data\n        S--)C: chat/contentReceived (assistant: text)\n\n    end\n    L-&gt;&gt;-S: Finish response\n    S-&gt;&gt;C: chat/contentReceived (system: finished)</code></pre> <p>Example of a tool call loop LLM interaction:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Client / Editor\n    participant S as ECA Server\n    participant L as LLM\n    C-&gt;&gt;S: chat/prompt\n    Note over C,S: ...&lt;br/&gt;Same as text flow\n    S-&gt;&gt;+L: Send prompt with&lt;br/&gt;available tools\n    loop LLM streaming / calling tools\n        Note right of L: Returns first `will`,&lt;br/&gt;then `check`, etc\n        L--)S: Stream data\n        S--)C: chat/contentReceived (assistant: text)\n        S--)C: chat/contentReceived (toolCallPrepare: name + args)\n        L-&gt;&gt;-S: Finish response:&lt;br/&gt;needs tool call&lt;br/&gt;'eca_directory_tree'\n        S-&gt;&gt;C: chat/contentReceived (toolCallRun)&lt;br/&gt;Ask user if should call tool\n        C--)S: chat/toolCallApprove\n        S-&gt;&gt;C: chat/contentReceived (toolCallRunning)\n        Note right of S: Call tool and get result\n        S-&gt;&gt;C: chat/contentReceived (toolCalled)\n        S-&gt;&gt;+L: Send previous prompt +&lt;br/&gt;LLM response +&lt;br/&gt;tool call result\n        Note right of L: Stream response\n    end\n    L-&gt;&gt;-S: Finish response\n    S-&gt;&gt;C: chat/contentReceived (system: finished)</code></pre>"},{"location":"protocol/#chat-prompt","title":"Chat Prompt (\u21a9\ufe0f)","text":"<p>A request sent from client to server, starting or continuing a chat in natural language as an agent. Used for broader questions or continuous discussion of project/files.</p> <p>Request: </p> <ul> <li>method: <code>chat/prompt</code></li> <li>params: <code>ChatPromptParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptParams {\n    /**\n     * The chat session identifier. If not provided, a new chat session will be created.\n     */\n    chatId?: string;\n\n    /**\n     * The message from the user in native language\n     */\n    message: string;\n\n    /**\n     * Specifies the AI model to be used for chat responses.\n     * Different models may have different capabilities, response styles,\n     * and performance characteristics.\n     */\n    model?: ChatModel;\n\n    /**\n     * The chat behavior used by server to handle chat communication and actions.\n     */\n    behavior?: ChatBehavior;\n\n    /**\n     * Optional contexts about the current workspace.\n     * Can include multiple different types of context.\n     */\n    contexts?: ChatContext[];\n}\n\n/**\n * The LLM model name.\n */\ntype ChatModel = string;\n\ntype ChatContext = FileContext | DirectoryContext | WebContext | RepoMapContext | CursorContext |McpResourceContext;\n\n/**\n * Context related to a file in the workspace\n */\ninterface FileContext {\n    type: 'file';\n    /**\n     * Path to the file\n     */\n    path: string;\n\n    /**\n     * Range of lines to retrive from file, if nil consider whole file.\n     */\n    linesRange?: LinesRange;\n}\n\ninterface LinesRange {\n   start: number;\n   end: number;\n}\n\n/**\n * Context related to a directory in the workspace\n */\ninterface DirectoryContext {\n    type: 'directory';\n    /**\n     * Path to the directory\n     */\n    path: string;\n}\n\n/**\n * Context related to web content\n */\ninterface WebContext {\n    type: 'web';\n    /**\n     * URL of the web content\n     */\n    url: string;\n}\n\n/**\n * Context about the workspaces repo-map, automatically calculated by server.\n * Clients should include this to chat by default but users may want exclude \n * this context to reduce context size if needed.\n *\n * @deprecated No longer needed, replaced by eca_directory_tree tool.\n */\ninterface RepoMapContext {\n    type: 'repoMap'; \n}\n\n/**\n * Context about the cursor position in editor, sent by client.\n * Clients should track path and cursor position.\n */\ninterface CursorContext {\n    type: 'cursor'; \n\n    /**\n     * File path of where the cursor is.\n     */\n    path: string;\n\n    /**\n     * Cursor position, if not using a selection start should be equal to end.\n     */\n    position: {\n       start: {\n           line: number;\n           character: number;\n       },\n       end: {\n           line: number;\n           character: number;\n       }\n    }\n}\n\n/***\n * A MCP resource available from a MCP server.\n */\ninterface McpResourceContext {\n    type: 'mcpResource';\n\n   /** \n    * The URI of the resource like file://foo/bar.clj\n    */\n    uri: string;\n\n    /** \n     * The name of the resource.\n     */\n    name: string;\n\n    /** \n     * The description of the resource.\n     */\n    description: string;\n\n    /** \n     * The mimeType of the resource like `text/markdown`.\n     */\n    mimeType: string;\n\n    /** \n     * The server name of this MCP resource.\n     */\n    server: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatPromptResponse {\n    /**\n     * Unique identifier for this chat session\n     */\n    chatId: string;\n\n    /*\n     * The model used for this chat request.\n     */\n    model: ChatModel;\n\n    /**\n     * What the server is doing after receing this prompt\n     */\n    status: 'prompting' | 'login';\n}\n</code></pre>"},{"location":"protocol/#chat-content-received","title":"Chat Content Received (\u2b05\ufe0f)","text":"<p>A server notification with a new content from the LLM.</p> <p>Notification: </p> <ul> <li>method: <code>chat/contentReceived</code></li> <li>params: <code>ChatContentReceivedParams</code> defined as follows:</li> </ul> <pre><code>interface ChatContentReceivedParams {\n    /**\n     * The chat session identifier this content belongs to\n     */\n    chatId: string;\n\n    /**\n     * The content received from the LLM\n     */\n    content: ChatContent;\n\n    /**\n     * The owner of this content.\n     */\n    role: 'user' | 'system' | 'assistant';\n}\n\n/**\n * Different types of content that can be received from the LLM\n */\ntype ChatContent = \n    | ChatTextContent \n    | ChatURLContent \n    | ChatProgressContent \n    | ChatUsageContent\n    | ChatReasonStartedContent \n    | ChatReasonTextContent \n    | ChatReasonFinishedContent \n    | ChatToolCallPrepareContent\n    | ChatToolCallRunContent\n    | ChatToolCallRunningContent\n    | ChatToolCalledContent\n    | ChatToolCallRejectedContent\n    | ChatMetadataContent;\n\n/**\n * Simple text message from the LLM\n */\ninterface ChatTextContent {\n    type: 'text';\n    /**\n     * The text content\n     */\n    text: string;\n}\n\n/**\n * Progress messages about the chat. \n * Usually to mark what eca is doing/waiting or tell it finished processing messages.\n */\ninterface ChatProgressContent {\n    type: 'progress';\n\n    /**\n     * The state of this progress.\n     */\n    state: 'running' | 'finished';\n\n    /*\n     * Extra text to show in chat about current state of this chat.\n     */\n    text: string;\n}\n\n/**\n * A reason started from the LLM\n *\n */\ninterface ChatReasonStartedContent {\n    type: 'reasonStarted';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n}\n\n/**\n * A reason text from the LLM\n *\n */\ninterface ChatReasonTextContent {\n    type: 'reasonText';\n\n    /**\n     * The id of a started reason\n     */\n    id: string;\n\n    /**\n     * The text content of the reasoning\n     */\n    text: string;\n}\n\n/**\n * A reason finished from the LLM\n *\n */\ninterface ChatReasonFinishedContent {\n    type: 'reasonFinished';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n\n    /**\n     * The total time the reason took in milliseconds.\n     */\n    totalTimeMs: number;\n}\n\n/**\n * URL content message from the LLM\n */\ninterface ChatURLContent {\n    type: 'url';\n\n    /**\n     * The URL title\n     */\n    title: string;\n\n    /**\n     * The URL link\n     */\n    url: string;\n}\n\n/**\n * Details about the chat's usage, like used tokens and costs.\n */\ninterface ChatUsageContent {\n    type: 'usage';\n\n    /**\n     * The total input + output tokens of the whole chat session so far.\n     */\n    sessionTokens: number;\n\n    /**\n     * The cost of the last sent message summing input + output tokens.\n     */\n    lastMessageCost?: string; \n\n    /**\n     * The cost of the whole chat session so far.\n     */\n    sessionCost?: string;\n\n    /**\n     * Informations about limits.\n     */\n    limit?: {\n        /**\n         * The context limit for this chat.\n         */\n        context: number;\n        /**\n         * The output limit for this chat.\n         */\n        output: number;\n    }\n}\n\n/**\n * Tool call that LLM is preparing to execute.\n * This will be sent multiple times for same tool id for each time LLM outputs \n * a part of the arg, so clients should append the arguments to UI.\n */\ninterface ChatToolCallPrepareContent {\n    type: 'toolCallPrepare';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Argument text of this tool call\n     */\n    argumentsText: string; \n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call that LLM will run, sent once per id.\n */\ninterface ChatToolCallRunContent {\n    type: 'toolCallRun';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * Whether this call requires manual approval from the user.\n     */\n    manualApproval: boolean;\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call that server is running to report to LLM later, sent once per id.\n */\ninterface ChatToolCallRunningContent {\n    type: 'toolCallRunning';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call result that LLM trigerred and was executed already, sent once per id.\n */\ninterface ChatToolCalledContent {\n    type: 'toolCalled';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: string[];\n\n    /**\n     * Whether it was a error\n     */\n    error: boolean;\n\n    /**\n     * the result of the tool call.\n     */\n    outputs: [{\n        /*\n         * The type of this output\n         */\n        type: 'text';\n\n        /**\n         * The content of this output\n         */\n        text: string; \n    }];\n\n    /**\n     * The total time the call took in milliseconds.\n     */\n    totalTimeMs: number;\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call rejected, sent once per id.\n */\ninterface ChatToolCallRejectedContent {\n    type: 'toolCallRejected';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * The reason why this tool call was rejected\n     */\n    reason: 'user-choice' | 'user-config';\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\ntype ToolCallOrigin = 'mcp' | 'native';\n\ntype ToolCallDetails = FileChangeDetails;\n\ninterface FileChangeDetails {\n    type: 'fileChange';\n\n     /**\n      * The file path of this file change\n      */\n     path: string;\n\n     /**\n      * The content diff of this file change\n      */\n     diff: string;\n\n     /**\n      * The count of lines added in this change.\n      */\n     linesAdded: number;\n\n     /**\n      * The count of lines removed in this change.\n      */\n     linesRemoved: number;\n}\n\n/**\n * Extra information about a chat\n */\ninterface ChatMetadataContent {\n    type: 'metadata';\n\n    /**\n     * The chat title.\n     */\n    title: string;\n}\n</code></pre>"},{"location":"protocol/#chat-approve-tool-call","title":"Chat approve tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to approve a waiting tool call. This will execute the tool call and continue the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallApprove</code></li> <li>params: <code>ChatToolCallApproveParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallApproveParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The approach to save this tool call.\n     */\n    save?: 'session';\n\n    /**\n     * The tool call identifier to approve.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-reject-tool-call","title":"Chat reject tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to reject a waiting tool call. This will not execute the tool call and return to the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallReject</code></li> <li>params: <code>ChatToolCallRejectParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallRejectParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The tool call identifier to reject.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-query-context","title":"Chat Query Context (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available contexts for user add to prompt calls.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryContext</code></li> <li>params: <code>ChatQueryContextParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryContextParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available contexts.\n     */\n    query: string;\n\n    /**\n     * The already considered contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryContextResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre>"},{"location":"protocol/#chat-query-commands","title":"Chat Query Commands (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available commands for user to call. Commands are multiple possible actions like MCP prompts, doctor, costs. Usually the  UX follows <code>/&lt;command&gt;</code> to spawn a command.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryCommands</code></li> <li>params: <code>ChatQueryCommandsParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryCommandsParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available commands.\n     */\n    query: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryCommandsResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available Commands.\n     */\n    commands: ChatCommand[];\n}\n\ninterface ChatCommand {\n    /**\n     * The name of the command.\n     */\n    name: string;\n\n    /**\n     * The description of the command.\n     */\n    description: string;\n\n    /**\n     * The type of this command\n     */\n    type: 'mcp-prompt' | 'native';\n\n    /**\n     * The arguments of the command.\n     */\n    arguments: [{\n       name: string;\n       description?: string;\n       required: boolean; \n    }];\n}\n</code></pre>"},{"location":"protocol/#chat-stop-prompt","title":"Chat stop prompt (\u27a1\ufe0f)","text":"<p>A client notification for server to stop the current chat prompt with LLM if running. This will stop LLM loops or ignore subsequent LLM responses so other prompts can be trigerred.</p> <p>Notification:</p> <ul> <li>method: <code>chat/promptStop</code></li> <li>params: <code>ChatPromptStopParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptStopParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n}\n</code></pre>"},{"location":"protocol/#chat-delete","title":"Chat delete (\u21a9\ufe0f)","text":"<p>A client request to delete a existing chat, removing all previous messages and used tokens/costs from memory, good for reduce context or start a new clean chat. After response, clients should reset chat UI to a clean state.</p> <p>Request: </p> <ul> <li>method: <code>chat/delete</code></li> <li>params: <code>ChatDeleteParams</code> defined as follows:</li> </ul> <pre><code>interface ChatDeleteParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n}\n</code></pre>"},{"location":"protocol/#chat-selected-behavior-changed","title":"Chat selected behavior changed (\u27a1\ufe0f)","text":"<p>A client notification for server telling the user selected a different behavior in chat.</p> <p>Notification:</p> <ul> <li>method: <code>chat/selectedBehaviorChanged</code></li> <li>params: <code>ChatSelectedBehaviorChanged</code> defined as follows:</li> </ul> <pre><code>interface ChatSelectedBehaviorChanged {\n    /**\n     * The selected behavior.\n     */\n    behavior: ChatBehavior;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatDeleteResponse {}\n</code></pre>"},{"location":"protocol/#editor-diagnostics","title":"Editor diagnostics (\u21aa\ufe0f)","text":"<p>A server request to retrieve LSP or any other kind of diagnostics if available from current workspaces. Useful for server to provide to LLM information about errors/warnings about current code.</p> <p>Request: </p> <ul> <li>method: <code>editor/getDiagnostics</code></li> <li>params: <code>EditorGetDiagnosticsParams</code> defined as follows:</li> </ul> <pre><code>interface EditorGetDiagnosticsParams {\n    /**\n     * Optional uri to get diagnostics, if nil return whole workspaces diagnostics.\n     */\n    uri?: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface EditorGetDiagnosticsResponse {\n    /**\n     * The list of diagnostics.\n     */\n    diagnostics: EditorDiagnostic[];\n}\n\ninterface EditorDiagnostic {\n    /**\n     * The diagnostic file uri.\n     */\n    uri: string;\n\n    /**\n     * The diagnostic severity.\n     */\n    severity: 'error' | 'warning' | 'info' | 'hint';\n\n    /**\n     * The diagnostic source. Ex: 'clojure-lsp'\n     */\n    source: string;\n\n    /**\n     * The diagnostic range (1-based).\n     */\n    range: {\n        start: {\n            line: number;\n            character: number;\n        };\n\n        end: {\n            line: number;\n            character: number;\n        };\n    };\n\n    /**\n     * The diagnostic code. Ex: 'wrong-args'\n     */\n    code?: string;\n\n    /**\n     * The diagnostic message. Ex: 'Wrong number of args for function X'\n     */\n    message: string; \n}\n</code></pre>"},{"location":"protocol/#completion","title":"Completion (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#edit","title":"Edit (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#configuration","title":"Configuration","text":""},{"location":"protocol/#config-updated","title":"Config updated (\u2b05\ufe0f)","text":"<p>A server notification with the new config server is considering (models, behaviors etc), usually related to config or auth changes. Clients should update UI accordingly, if a field is missing/null, means it had no change since last config updated, so clients should ignore.</p> <p>Notification: </p> <ul> <li>method: <code>config/updated</code></li> <li>params: <code>configUpdatedParams</code> defined as follows:</li> </ul> <pre><code>interface ConfigUpdatedParams {\n    /**\n     * Configs related to chat.\n     */\n    chat?: {\n\n       /**\n        * The models the user can use in chat.\n        */\n        models?: ChatModel[];\n\n        /**\n        * The chat behaviors the user can select.\n        */\n        behaviors?: ChatBehavior[];\n\n        /**\n         * The model for client select in chat, if that is present\n         * clients should forcefully update chat selected model.\n         * \n         * Server returns this when starting and only when makes sense to \n         * force update a model, like a config change.\n         */\n        selectModel?: ChatModel;\n\n        /**\n         * The behavior for client select in chat, if that is present\n         * clients should forcefully update chat selected behavior.\n         * \n         * Server returns this when starting and only when makes sense to \n         * force update a behavior, like a config change.\n         */\n        selectBehavior?: ChatBehavior;\n\n        /**\n        * Message to show when starting a new chat.\n        */\n        welcomeMessage?: string;\n    }\n}\n</code></pre>"},{"location":"protocol/#tool-updated","title":"Tool updated (\u2b05\ufe0f)","text":"<p>A server notification about a tool status update like a MCP or native tool. This is useful for clients present to user the list of configured tools/MCPs, their status and available tools and actions.</p> <p>Notification: </p> <ul> <li>method: <code>tool/serverUpdated</code></li> <li>params: <code>ToolServerUpdatedParams</code> defined as follows:</li> </ul> <pre><code>type ToolServerUpdatedParams = EcaServerUpdatedParams | MCPServerUpdatedParams;\n\ninterface EcaServerUpdatedParams {\n    type: 'native';\n\n    name: 'ECA';\n\n    status: 'running';\n\n    /**\n     * The built-in tools supported by eca.\n     */\n    tools: ServerTool[];\n}\n\ninterface MCPServerUpdatedParams {\n    type: 'mcp';\n\n    /**\n     * The server name.\n     */\n    name: string;\n\n    /**\n     * The command to start this server.\n     */\n    command: string;\n\n    /**\n     * The arguments to start this server.\n     */\n    args: string[];\n\n    /**\n     * The status of the server.\n     */\n    status: 'running' | 'starting' | 'stopped' | 'failed' | 'disabled';\n\n    /**\n     * The tools supported by this mcp server if not disabled.\n     */\n    tools?: ServerTool[];\n}\n\ninterface ServerTool {\n    /**\n     * The server tool name.\n     */\n    name: string;\n\n    /**\n     * The server tool description.\n     */\n    description: string;\n\n    /**\n     * The server tool parameters.\n     */\n    parameters: any; \n\n    /**\n     * Whther this tool is disabled.\n     */\n    disabled?: boolean;\n}\n</code></pre>"},{"location":"protocol/#stop-mcp-server","title":"Stop MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to stop a MCP server, stopping the process. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/stopServer</code></li> <li>params: <code>MCPStopServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStopServerParams {\n    /**\n     * The MCP server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#start-mcp-server","title":"Start MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to start a stopped MCP server, starting the process again. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/startServer</code></li> <li>params: <code>MCPStartServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStartServerParams {\n    /**\n     * The server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#add-mcp","title":"Add MCP (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#general-features","title":"General features","text":""},{"location":"protocol/#showmessage","title":"showMessage (\u2b05\ufe0f)","text":"<p>A notification from server telling client to present a message to user.</p> <p>Request: </p> <ul> <li>method: <code>$/showMessage</code></li> <li>params: <code>ShowMessageParams</code> defined as follows:</li> </ul> <pre><code>interface ShowMessageParams {\n    /**\n     * The message type. See {@link MessageType}.\n    */\n    type: MessageType;\n\n    /**\n     * The actual message.\n     */\n    message: string;\n}\n\nexport type MessageType = 'error' | 'warning' | 'info';\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#logs-stderr","title":"Logs (stderr)","text":"<p>All supported editors have options to set the server args to help with that and options to check the server logs.</p> <p>To access the server logs:</p> EmacsVsCodeIntelliJNvim <p><code>M-x</code> <code>eca-show-stderr</code></p> <p>Check the output channel <code>ECA stderr</code>.</p> <p>Via action 'ECA: Show server logs'.</p> <p><code>EcaShowLogs</code> </p>"},{"location":"troubleshooting/#server-logs","title":"Server logs","text":"<p>This controls what's logged by server on its actions, you can control to log more things via <code>--log-level debug</code> server arg. This should help log LLM outputs, and other useful stuff.</p>"},{"location":"troubleshooting/#client-server-logs","title":"Client&lt;-&gt;Server logs","text":"<p>ECA works with clients (editors) sending and receiving messages to server, a process, you can start server <code>--verbose</code> which should log all jsonrpc communication between client and server to <code>stderr</code> buffer like what is being sent to LLMs or what ECA is responding to editors. </p>"},{"location":"troubleshooting/#doctor-command","title":"Doctor command","text":"<p><code>/doctor</code> command should log useful information to debug model used, server version, env vars and more.</p>"},{"location":"troubleshooting/#missing-env-vars","title":"Missing env vars","text":"<p>When launching editors from a GUI application (Dock, Applications folder, or desktop environment), high chance that it won't inherit environment variables from your shell configuration files (<code>.zshrc</code>, <code>.bashrc</code>, etc.). Since the ECA server is started as a subprocess from editor, it inherits the editor environment, which may be missing your API keys and other configuration.</p> <p>You can check if the env vars are available via <code>/doctor</code>.</p> <p>One way to workaround that is to start the editor from your terminal.</p>"},{"location":"troubleshooting/#alternatives","title":"Alternatives","text":"<ul> <li> <p>Start the editor from your terminal.</p> </li> <li> <p>Set variables in your editor if supported, example in Emacs: <code>(setenv \"MY_ENV\" \"my-value\")</code></p> </li> <li> <p>On macOS, you can set environment variables system-wide using <code>launchctl</code>:</p> </li> </ul> <pre><code>launchctl setenv ANTHROPIC_API_KEY \"your-key-here\"\n</code></pre>"},{"location":"troubleshooting/#ask-for-help","title":"Ask for help","text":"<p>You can ask for help via chat here</p>"}]}